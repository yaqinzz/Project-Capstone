# -*- coding: utf-8 -*-
"""Projek Capstone_Klasifikasi Gambar Pneumonia_Model Sequensial.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ttAyvUgy_X-iu11S14uEp2eKVGbESur8

# Project Capstone : Sistem Deteksi Pneumonia

#### Import Library
"""

# Import Library
import os
import numpy as np
import matplotlib.pyplot as plt
from IPython.display import Image, display
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.preprocessing import image
from tensorflow.keras import layers, models

!pip install kagglehub
import kagglehub

"""#### Load Dataset"""

# Download dataset
path = kagglehub.dataset_download("paultimothymooney/chest-xray-pneumonia")

print("Path to dataset files:", path)

# Direktori Dataset
data_dir = "/kaggle/input/chest-xray-pneumonia"

# Direktori data train, val, dan test
train_dir = os.path.join(path, "chest_xray/train")
val_dir = os.path.join(path, "chest_xray/val")
test_dir = os.path.join(path, "chest_xray/test")

# Lihat label yang tersedia (subfolder dalam train_dir)
classes = os.listdir(train_dir)
print("Label yang tersedia:", classes)

"""#### Data Wrangling"""

# Menampilkan beberapa gambar dari masing-masing kelas
for label in classes:
    folder_path = os.path.join(train_dir, label)
    # Ensure the path exists before listing files
    if os.path.exists(folder_path):
        images = os.listdir(folder_path)

        print(f"\nContoh gambar dari kelas: {label}")

        # Tampilkan 3 gambar pertama
        for image_name in images[:3]:
            image_path = os.path.join(folder_path, image_name)
            # Check if the file is an image before trying to display
            if image_path.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):
                 display(Image(filename=image_path))
    else:
        print(f"Warning: Directory not found for label: {label}")

# Menampilkan jumlah data pada masing-masing kelas
print("\nJumlah data pada masing-masing kelas:")
for label in classes:
    folder_path = os.path.join(train_dir, label)
    if os.path.exists(folder_path):
        # Hitung jumlah file dalam folder
        num_images = len([name for name in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, name))])
        print(f"- {label}: {num_images} data")
    else:
        print(f"Warning: Directory not found for label: {label}")

# Menampilkan jumlah kelas
print("Jumlah kelas:", len(classes))

# Buat bar plot kelas
plt.figure(figsize=(8, 6))
plt.bar(classes, [len(os.listdir(os.path.join(train_dir, label))) for label in classes])
plt.xlabel("Kelas")
plt.ylabel("Jumlah Data")

"""Insight : Dataset berisi 2 kelas yaitu pneumonia dan normal, secara umum dataset ini imbalance karena kelas pneumonia lebih banyak dibandingkan kelas normal."""

import os
import matplotlib.pyplot as plt

# Kelas (pastikan sesuai isi direktori)
classes = ['NORMAL', 'PNEUMONIA']

# Path dataset
path = "/kaggle/input/chest-xray-pneumonia" # ubah sesuai path root jika perlu
train_dir = os.path.join(path, "chest_xray/train")
val_dir = os.path.join(path, "chest_xray/val")
test_dir = os.path.join(path, "chest_xray/test")

# Fungsi untuk hitung jumlah data per kelas
def count_images(directory, classes):
    counts = {}
    for cls in classes:
        dir_path = os.path.join(directory, cls)
        if os.path.exists(dir_path):
            counts[cls] = len([
                f for f in os.listdir(dir_path) if os.path.isfile(os.path.join(dir_path, f))
            ])
        else:
            counts[cls] = 0
    return counts

# Hitung
train_counts = count_images(train_dir, classes)
val_counts = count_images(val_dir, classes)
test_counts = count_images(test_dir, classes)

# Bar plot
x = range(len(classes))
width = 0.25

plt.figure(figsize=(10, 6))

# Plot batang
bars1 = plt.bar([i - width for i in x], [train_counts[c] for c in classes], width, label='Train')
bars2 = plt.bar(x, [val_counts[c] for c in classes], width, label='Validation')
bars3 = plt.bar([i + width for i in x], [test_counts[c] for c in classes], width, label='Test')

# Tambahkan label jumlah di atas masing-masing bar
def add_bar_labels(bars):
    for bar in bars:
        height = bar.get_height()
        plt.annotate(f'{height}',
                     xy=(bar.get_x() + bar.get_width() / 2, height),
                     xytext=(0, 5),  # offset label ke atas
                     textcoords="offset points",
                     ha='center', va='bottom', fontsize=9)

add_bar_labels(bars1)
add_bar_labels(bars2)
add_bar_labels(bars3)

# Detail plot
plt.xlabel("Kelas")
plt.ylabel("Jumlah Data")
plt.title("Distribusi Data Train, Validation, dan Test per Kelas")
plt.xticks(x, classes)
plt.legend()
plt.tight_layout()
plt.show()

"""#### Data Preprocessing

##### Data Merging (Penggabungan kembali data train, val, dan test)
"""

# Penggabungan kembali data train, data validation, dan data test untuk dilakukan splitting ulang
import os
import shutil

# Direktori asli
data_dir = "/kaggle/input/chest-xray-pneumonia"
train_dir = os.path.join(data_dir, "chest_xray/train")
val_dir = os.path.join(data_dir, "chest_xray/val")
test_dir = os.path.join(data_dir, "chest_xray/test")

# Direktori baru untuk dataset gabungan
merged_dir = "/kaggle/working/merged_dataset"
os.makedirs(merged_dir, exist_ok=True)

# Subfolder untuk setiap kelas (PNEUMONIA dan NORMAL)
for class_name in ["NORMAL", "PNEUMONIA"]:
    os.makedirs(os.path.join(merged_dir, class_name), exist_ok=True)

# Fungsi untuk menggabungkan file dari sumber ke target
def merge_data(source_dir, target_dir):
    for class_name in os.listdir(source_dir):
        class_source = os.path.join(source_dir, class_name)
        class_target = os.path.join(target_dir, class_name)
        for file_name in os.listdir(class_source):
            source_file = os.path.join(class_source, file_name)
            target_file = os.path.join(class_target, file_name)
            shutil.copy2(source_file, target_file)  # Salin dengan metadata

# Gabungkan train, val, dan test
merge_data(train_dir, merged_dir)
merge_data(val_dir, merged_dir)
merge_data(test_dir, merged_dir)

print("Data berhasil digabungkan di:", merged_dir)

# Hitung total data gabungan per kelas
for class_name in ["NORMAL", "PNEUMONIA"]:
    class_dir = os.path.join(merged_dir, class_name)
    print(f"Jumlah data {class_name}: {len(os.listdir(class_dir))}")

"""##### Data Cleaning"""

import os

def check_duplicate_filenames(directory):
    filenames = []
    duplicates = []

    for root, _, files in os.walk(directory):
        for file in files:
            if file in filenames:
                duplicates.append(os.path.join(root, file))
            else:
                filenames.append(file)

    return duplicates

# Cek data duplikat berdasarkan nama file
merged_dir = "/kaggle/working/merged_dataset"
duplicates = check_duplicate_filenames(train_dir)
print(f"Jumlah duplikat berdasarkan nama: {len(duplicates)}")
print("Contoh duplikat:", duplicates[:3])

# Cek data duplikat berdasarkan hash konten
import hashlib
from collections import defaultdict # Import defaultdict

def calculate_hash(filepath, block_size=65536):
    hasher = hashlib.md5()
    with open(filepath, 'rb') as f:
        buf = f.read(block_size)
        while len(buf) > 0:
            hasher.update(buf)
            buf = f.read(block_size)
    return hasher.hexdigest()

def check_duplicate_content(directory):
    hash_dict = defaultdict(list)

    for root, _, files in os.walk(directory):
        for file in files:
            filepath = os.path.join(root, file)
            # Pastikan ini adalah file sebelum mencoba membaca
            if os.path.isfile(filepath):
                file_hash = calculate_hash(filepath)
                hash_dict[file_hash].append(filepath)

    # Filter hanya hash dengan >1 file
    duplicates = {hash_val: paths for hash_val, paths in hash_dict.items() if len(paths) > 1}
    return duplicates

# Contoh penggunaan
# Menggunakan merged_dir yang sudah didefinisikan di sel sebelumnya
merged_dir = "/kaggle/working/merged_dataset" # Pastikan variabel ini ada di sini jika sel sebelumnya tidak dieksekusi
duplicates_content = check_duplicate_content(merged_dir) # Ganti merge_dir menjadi merged_dir
print(f"Jumlah file dengan konten identik: {len(duplicates_content)}")
if duplicates_content:
    # Menampilkan contoh duplikat
    # Iterasi melalui item pertama jika ada
    first_duplicate_hash = list(duplicates_content.keys())[0]
    print("Contoh duplikat (berdasarkan hash konten):", duplicates_content[first_duplicate_hash][:2])

# Tampilkan visualisasi gambar duplikat
from IPython.display import Image, display
import os

print("Menampilkan contoh gambar duplikat (berdasarkan hash konten):")

# Batasi jumlah set duplikat yang ditampilkan agar output tidak terlalu panjang
num_sets_to_display = 30

# Ambil kunci hash dari duplikat yang ditemukan
duplicate_hashes = list(duplicates_content.keys())

# Iterasi melalui set duplikat
for i, hash_val in enumerate(duplicate_hashes[:num_sets_to_display]):
    paths = duplicates_content[hash_val]

    # Pastikan ada lebih dari satu file dalam set ini (seharusnya selalu ada karena itu definisi duplikat)
    if len(paths) > 1:
        print(f"\nSet Duplikat {i+1} (Hash: {hash_val[:8]}...):")

        # Tampilkan beberapa gambar pertama dari set duplikat ini (misal 2 gambar)
        for j, image_path in enumerate(paths[:2]): # Ambil 2 gambar pertama dari set duplikat
             # Tambahkan pengecekan ekstensi file seperti pada sel sebelumnya
            if os.path.isfile(image_path) and image_path.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):
                print(f"  - {os.path.basename(image_path)}") # Tampilkan nama file
                try:
                    display(Image(filename=image_path, width=150, height=150)) # Tampilkan gambar
                except Exception as e:
                    print(f"Could not display image {image_path}: {e}")
            elif not os.path.isfile(image_path):
                 print(f"Warning: Path is not a file: {image_path}")
            else:
                 print(f"Warning: File does not seem to be an image: {image_path}")

print(f"\nMenampilkan {min(len(duplicate_hashes), num_sets_to_display)} set duplikat pertama.")

# Hapus data duplikat
import os
from collections import defaultdict
import hashlib

# 1. Hitung duplikat konten (sebelum penghapusan)
def calculate_hash(filepath, block_size=65536):
    hasher = hashlib.md5()
    with open(filepath, 'rb') as f:
        buf = f.read(block_size)
        while len(buf) > 0:
            hasher.update(buf)
            buf = f.read(block_size)
    return hasher.hexdigest()

def check_duplicate_content(directory):
    hash_dict = defaultdict(list)
    for root, _, files in os.walk(directory):
        for file in files:
            filepath = os.path.join(root, file)
            file_hash = calculate_hash(filepath)
            hash_dict[file_hash].append(filepath)
    return {hash_val: paths for hash_val, paths in hash_dict.items() if len(paths) > 1}

# 2. Hapus duplikat
merged_dir = 'path/to/merged_dir'  # Pastikan path benar (gunakan os.path.abspath)
duplicates_content = check_duplicate_content(merged_dir)  # Hitung duplikat
total_deleted = 0

for hash_val, file_list in duplicates_content.items():
    if len(file_list) > 1:
        files_to_delete = file_list[1:]  # Simpan file pertama, hapus sisanya
        for file_path in files_to_delete:
            try:
                if os.path.exists(file_path):
                    os.remove(file_path)
                    total_deleted += 1
                    print(f"Dihapus: {file_path}")
                else:
                    print(f"File tidak ditemukan: {file_path}")
            except OSError as e:
                print(f"Gagal menghapus {file_path}: {e}")

print(f"\nTotal {total_deleted} file duplikat dihapus dari {merged_dir}.")

import os
from collections import defaultdict
import hashlib

# Fungsi untuk menghitung hash file
def calculate_hash(filepath, block_size=65536):
    hasher = hashlib.md5()
    with open(filepath, 'rb') as f:
        buf = f.read(block_size)
        while len(buf) > 0:
            hasher.update(buf)
            buf = f.read(block_size)
    return hasher.hexdigest()

# Fungsi untuk mencari file duplikat
def check_duplicate_content(directory):
    hash_dict = defaultdict(list)
    for root, _, files in os.walk(directory):
        for file in files:
            filepath = os.path.join(root, file)
            # Ensure it's a file before calculating hash
            if os.path.isfile(filepath):
                file_hash = calculate_hash(filepath)
                hash_dict[file_hash].append(filepath)
    return {hash_val: paths for hash_val, paths in hash_dict.items() if len(paths) > 1}

# Fungsi untuk menghitung jumlah file per kelas
def count_files_per_class(directory):
    class_counts = {}
    # List directories first to avoid issues with non-directory files
    for class_name in os.listdir(directory):
        class_dir = os.path.join(directory, class_name)
        if os.path.isdir(class_dir):
            # Count files only within the class directory
            class_counts[class_name] = len([name for name in os.listdir(class_dir) if os.path.isfile(os.path.join(class_dir, name))])
    return class_counts

# 1. Hitung jumlah file awal
merged_dir = '/kaggle/working/merged_dataset'  # Corrected path
print("Jumlah file sebelum penghapusan:")
initial_counts = count_files_per_class(merged_dir)
for class_name, count in initial_counts.items():
    print(f"{class_name}: {count}")

# 2. Process duplicate deletion
duplicates_content = check_duplicate_content(merged_dir)
total_deleted = 0

for hash_val, file_list in duplicates_content.items():
    if len(file_list) > 1:
        files_to_delete = file_list[1:]  # Keep the first file, delete the rest
        for file_path in files_to_delete:
            try:
                # Explicitly check if the file exists before attempting to delete
                if os.path.exists(file_path) and os.path.isfile(file_path):
                    os.remove(file_path)
                    total_deleted += 1
                    print(f"Dihapus: {file_path}")
                elif not os.path.exists(file_path):
                    print(f"File tidak ditemukan (sudah dihapus atau tidak ada): {file_path}")
                # No need for an else for non-files as check_duplicate_content filters for files
            except OSError as e:
                print(f"Gagal menghapus {file_path}: {e}")

# 3. Count files after deletion
print("\nJumlah file setelah penghapusan:")
final_counts = count_files_per_class(merged_dir)
for class_name, count in final_counts.items():
    print(f"{class_name}: {count}")

print(f"\nTotal {total_deleted} file duplikat dihapus dari {merged_dir}.")

"""##### Data Splitting"""

import os
import shutil
from sklearn.model_selection import train_test_split

# Direktori dataset gabungan yang sudah dibersihkan
merged_dir = '/kaggle/working/merged_dataset'

# Direktori baru untuk data split
base_split_dir = '/kaggle/working/split_dataset'
train_split_dir = os.path.join(base_split_dir, 'train')
val_split_dir = os.path.join(base_split_dir, 'val')
test_split_dir = os.path.join(base_split_dir, 'test')

# Buat direktori untuk setiap split dan kelas
for directory in [train_split_dir, val_split_dir, test_split_dir]:
    for class_name in ["NORMAL", "PNEUMONIA"]:
        os.makedirs(os.path.join(directory, class_name), exist_ok=True)

# Kumpulkan semua path file dan label
all_filepaths = []
all_labels = []
for class_name in ["NORMAL", "PNEUMONIA"]:
    class_dir = os.path.join(merged_dir, class_name)
    for filename in os.listdir(class_dir):
        filepath = os.path.join(class_dir, filename)
        if os.path.isfile(filepath): # Pastikan ini file
            all_filepaths.append(filepath)
            all_labels.append(class_name)

# Split data menjadi train (80%), validation (10%), dan test (10%)
# Pertama, split train+validation dan test
train_val_filepaths, test_filepaths, train_val_labels, test_labels = train_test_split(
    all_filepaths, all_labels, test_size=0.1, stratify=all_labels, random_state=42
)

# Kedua, split train+validation menjadi train dan validation
train_filepaths, val_filepaths, train_labels, val_labels = train_test_split(
    train_val_filepaths, train_val_labels, test_size=0.111, stratify=train_val_labels, random_state=42 # 0.111 ~= 0.1 / 0.9
)


# Fungsi untuk menyalin file
def copy_files(filepaths, labels, target_base_dir):
    for filepath, label in zip(filepaths, labels):
        filename = os.path.basename(filepath)
        target_path = os.path.join(target_base_dir, label, filename)
        shutil.copy2(filepath, target_path) # Salin dengan metadata

# Salin file ke direktori yang sesuai
print("Menyalin file ke direktori split...")
copy_files(train_filepaths, train_labels, train_split_dir)
copy_files(val_filepaths, val_labels, val_split_dir)
copy_files(test_filepaths, test_labels, test_split_dir)

print("\nData berhasil di-split dan disalin:")
print(f"Train data di: {train_split_dir}")
print(f"Validation data di: {val_split_dir}")
print(f"Test data di: {test_split_dir}")

# Hitung jumlah data di setiap split
print("\nJumlah data di setiap split:")
print(f"Train - NORMAL: {len(os.listdir(os.path.join(train_split_dir, 'NORMAL')))} data")
print(f"Train - PNEUMONIA: {len(os.listdir(os.path.join(train_split_dir, 'PNEUMONIA')))} data")
print(f"Validation - NORMAL: {len(os.listdir(os.path.join(val_split_dir, 'NORMAL')))} data")
print(f"Validation - PNEUMONIA: {len(os.listdir(os.path.join(val_split_dir, 'PNEUMONIA')))} data")
print(f"Test - NORMAL: {len(os.listdir(os.path.join(test_split_dir, 'NORMAL')))} data")
print(f"Test - PNEUMONIA: {len(os.listdir(os.path.join(test_split_dir, 'PNEUMONIA')))} data")

import os
import glob
import pandas as pd

# Path ke folder masing-masing split
train_dir = '/kaggle/working/split_dataset/train'
val_dir = '/kaggle/working/split_dataset/val'
test_dir = '/kaggle/working/split_dataset/test'

def create_dataframe_from_directory(directory):
    # Ambil semua file dari subfolder (diasumsikan per kelas)
    filepaths = glob.glob(os.path.join(directory, '*', '*'))
    labels = [os.path.basename(os.path.dirname(path)) for path in filepaths]
    df = pd.DataFrame({'filepaths': filepaths, 'labels': labels})
    return df

# Pastikan direktori ada dan buat DataFrame
if os.path.exists(train_dir):
    train_df = create_dataframe_from_directory(train_dir)
    print("Train Data:")
    print(train_df['labels'].value_counts(), "\n")
else:
    print(f"Folder train tidak ditemukan di: {train_dir}")

if os.path.exists(val_dir):
    val_df = create_dataframe_from_directory(val_dir)
    print("Validation Data:")
    print(val_df['labels'].value_counts(), "\n")
else:
    print(f"Folder validation tidak ditemukan di: {val_dir}")

if os.path.exists(test_dir):
    test_df = create_dataframe_from_directory(test_dir)
    print("Test Data:")
    print(test_df['labels'].value_counts(), "\n")
else:
    print(f"Folder test tidak ditemukan di: {test_dir}")

"""##### Data Augmentation pada Data Train dan Data Preprocessing pada Data Validation dan Test"""

from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.preprocessing import LabelEncoder
import numpy as np

# Ukuran gambar dan batch size
img_size = (224, 224)
batch_size = 32
# === Hitung test_batch_size agar tidak terlalu besar ===
ts_length = len(test_df)
test_batch_size = max(sorted([ts_length // n for n in range(1, ts_length + 1)
                              if ts_length % n == 0 and ts_length / n <= 80]))
test_steps = ts_length // test_batch_size

# === Data Augmentation untuk Training ===
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=15,
    width_shift_range=0.1,
    height_shift_range=0.1,
    shear_range=0.1,
    zoom_range=0.1,
    horizontal_flip=True,
    brightness_range=(0.8, 1.2),
    fill_mode='nearest'
)

# === Validasi & Test hanya Rescale ===
valid_test_datagen = ImageDataGenerator(rescale=1./255)

# === Data Generator dari DataFrame ===
train_gen = train_datagen.flow_from_dataframe(
    dataframe=train_df,
    x_col='filepaths',
    y_col='labels',
    target_size=img_size,
    class_mode='categorical',
    batch_size=batch_size,
    shuffle=True
)

val_gen = valid_test_datagen.flow_from_dataframe(
    dataframe=val_df,
    x_col='filepaths',
    y_col='labels',
    target_size=img_size,
    class_mode='categorical',
    batch_size=batch_size,
    shuffle=True
)

test_gen = valid_test_datagen.flow_from_dataframe(
    dataframe=test_df,
    x_col='filepaths',
    y_col='labels',
    target_size=img_size,
    class_mode='categorical',
    batch_size=test_batch_size,
    shuffle=False
)

# === Optional: class weights ===
le = LabelEncoder()
y_train_encoded = le.fit_transform(train_df['labels'].values)

from sklearn.utils.class_weight import compute_class_weight

class_weights = compute_class_weight(
    class_weight='balanced',
    classes=np.unique(y_train_encoded),
    y=y_train_encoded
)

class_weights_dict = dict(enumerate(class_weights))

"""#### Modelling with CNN-Sequential"""

!pip install keras-tuner --upgrade

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import (Input, Conv2D, MaxPooling2D, GlobalAveragePooling2D,
                                     Dense, Dropout, BatchNormalization)
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l2
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Bangun model CNN
def build_model():
    model = Sequential()
    model.add(Input(shape=(224, 224, 3)))

    # Regularisasi
    l2_lambda = 1e-4

    # Conv Block 1
    model.add(Conv2D(64, kernel_size=3, activation='relu', padding='same', kernel_regularizer=l2(l2_lambda)))
    model.add(BatchNormalization())
    model.add(MaxPooling2D())
    model.add(Dropout(0.2))

    # Conv Block 2
    model.add(Conv2D(128, kernel_size=3, activation='relu', padding='same', kernel_regularizer=l2(l2_lambda)))
    model.add(BatchNormalization())
    model.add(MaxPooling2D())
    model.add(Dropout(0.3))

    # Conv Block 3
    model.add(Conv2D(256, kernel_size=3, activation='relu', padding='same', kernel_regularizer=l2(l2_lambda)))
    model.add(BatchNormalization())
    model.add(MaxPooling2D())
    model.add(Dropout(0.4))

    # Head
    model.add(GlobalAveragePooling2D())
    model.add(Dense(256, activation='relu', kernel_regularizer=l2(l2_lambda)))
    model.add(BatchNormalization())
    model.add(Dropout(0.5))
    model.add(Dense(2, activation='softmax'))  # Output 2 kelas

    # Kompilasi
    optimizer = Adam(learning_rate=1e-4)
    model.compile(optimizer=optimizer,
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])
    return model

model = build_model()
model.summary()

from tensorflow.keras.callbacks import Callback, ReduceLROnPlateau, EarlyStopping, ModelCheckpoint

class StopAtValAccuracy(Callback):
    def __init__(self, target=0.9):
        super().__init__()
        self.target = target

    def on_epoch_end(self, epoch, logs=None):
        val_acc = logs.get('val_accuracy')
        if val_acc is not None:
            if val_acc >= self.target:
                print(f"\n✅ Validation accuracy {val_acc:.4f} reached target {self.target:.2f}. Stopping training.")
                self.model.stop_training = True

# Daftar callback
callbacks = [
    ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=3,
        min_lr=1e-6,
        verbose=1
    ),
    EarlyStopping(
        monitor='val_loss',
        patience=10,
        restore_best_weights=True,
        verbose=1
    ),
    ModelCheckpoint(
        'best_model.h5',
        save_best_only=True,
        monitor='val_accuracy',
        verbose=1
    ),
    StopAtValAccuracy(target=0.90)
]

# Bangun dan latih model
model = build_model()

history = model.fit(
    train_gen,
    epochs=50,
    validation_data=val_gen,
    steps_per_epoch=len(train_gen),
    validation_steps=len(val_gen),
    class_weight=class_weights_dict,
    callbacks=callbacks
)

from google.colab import drive
drive.mount('/content/drive')

# Buat direktori jika belum ada
import os
save_path = '/content/drive/MyDrive/Project Capstone/Best_Model'
os.makedirs(save_path, exist_ok=True)

# Simpan model dalam format .h5 ke Google Drive
model.save(os.path.join(save_path, 'best_pneumonia_model.h5'))

"""#### Model Evaluation"""

from google.colab import drive
drive.mount('/content/drive')

# === 1. Import Library ===
import tensorflow as tf
from tensorflow.keras.models import load_model
import numpy as np
import matplotlib.pyplot as plt

# === 2. Load Model Terbaik ===
model = load_model("/content/drive/MyDrive/Project Capstone/Best_Model/best_pneumonia_model.h5")

# === 3. Evaluasi Model pada Data Test ===
test_loss, test_accuracy = model.evaluate(test_gen, steps=len(test_gen), verbose=1)
print(f"\n📊 Test Accuracy: {test_accuracy:.4f}")
print(f"📉 Test Loss: {test_loss:.4f}")

# === 4. Ambil Riwayat Training ===
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs_range = range(len(acc))

# === 5. Plot Akurasi dan Loss ===
plt.figure(figsize=(12, 5))

# Plot Accuracy
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.axhline(y=test_accuracy, color='red', linestyle='--', label='Test Accuracy')
plt.title('Training, Validation, and Test Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(loc='lower right')

# Plot Loss
plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.axhline(y=test_loss, color='red', linestyle='--', label='Test Loss')
plt.title('Training, Validation, and Test Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend(loc='upper right')

plt.tight_layout()
plt.show()

from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import numpy as np

# === 1. Prediksi pada test set ===
# Pastikan test_gen tidak di-shuffle saat didefinisikan
predictions = model.predict(test_gen, steps=len(test_gen), verbose=1)

# Ambil indeks kelas hasil prediksi (probabilitas → label prediksi)
predicted_class_indices = np.argmax(predictions, axis=1)

# Ambil label asli dari test set
true_class_indices = test_gen.classes

# Ambil label nama kelas (string) dari generator
class_labels = list(test_gen.class_indices.keys())

# === 2. Confusion Matrix ===
cm = confusion_matrix(true_class_indices, predicted_class_indices)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)

plt.figure(figsize=(6, 6))
disp.plot(cmap=plt.cm.Blues, values_format='d')
plt.title("Confusion Matrix")
plt.grid(False)
plt.show()

# === 3. Classification Report ===
report = classification_report(true_class_indices, predicted_class_indices, target_names=class_labels)
print("=== Classification Report ===\n")
print(report)

"""#### Inferensi dengan gambar diluar dataset"""

from google.colab import files  # Untuk Colab
# Jika di Kaggle, gunakan dari panel file uploader
uploaded = files.upload()

from tensorflow.keras.preprocessing import image
from tensorflow.keras.models import load_model  # Pastikan load_model diimpor
import numpy as np
import matplotlib.pyplot as plt
from google.colab import files # Import files for Colab upload

# Cek apakah file berhasil di-upload
if 'uploaded' in locals() and uploaded:
    img_filename = list(uploaded.keys())[0]

    # Load model
    try:
        # Mengganti load_mode menjadi load_model
        model = load_model("/content/drive/MyDrive/Project Capstone/Best_Model/best_pneumonia_model.h5")
        print("✅ Model loaded successfully.")
    except Exception as e:
        print(f"❌ Error loading model: {e}")
        # Tidak perlu raise lagi setelah menampilkan error
        # raise

    # Ukuran input gambar (ubah jika model Anda berbeda)
    img_size = (224, 224)

    try:
        # Preprocessing gambar
        img = image.load_img(img_filename, target_size=img_size)
        img_array = image.img_to_array(img) / 255.0
        img_array = np.expand_dims(img_array, axis=0)

        # Prediksi
        prediction = model.predict(img_array)  # Output shape: (1, 2)
        predicted_class = np.argmax(prediction[0])  # Ambil indeks kelas dengan probabilitas tertinggi
        confidence = prediction[0][predicted_class]

        # Label kelas (urutannya harus sama seperti saat training)
        # Pastikan urutan label ini sesuai dengan urutan kelas saat training (saat membuat generator)
        # Dalam kasus ini, berdasarkan output flow_from_dataframe sebelumnya, urutannya biasanya ['NORMAL', 'PNEUMONIA']
        class_labels = ['NORMAL', 'PNEUMONIA']
        predicted_label = class_labels[predicted_class]

        # Visualisasi hasil
        plt.imshow(img)
        plt.axis('off')
        plt.title(f"Prediksi: {predicted_label} ({confidence*100:.2f}%)")
        plt.show()

    except Exception as e:
        print(f"❌ Error during inference: {e}")
else:
    print("❌ Tidak ada file yang di-upload.")

"""### Save Model"""

!pip install tensorflowjs

from google.colab import drive
drive.mount('/content/drive')

import os
import tensorflow as tf
from tensorflow import keras
import tensorflowjs as tfjs

# Direktori penyimpanan di Google Drive
save_dir = "/content/drive/MyDrive/Project Capstone/Best_Model"
os.makedirs(save_dir, exist_ok=True)
print(f"📁 Direktori penyimpanan: {save_dir}")

# --- 1. Simpan format .keras (rekomendasi Keras 3) ---
keras_path = os.path.join(save_dir, 'model_pneumonia.keras')
try:
    model.save(keras_path)
    print(f"✅ Model disimpan dalam format .keras di: {keras_path}")
except Exception as e:
    print(f"❌ [Keras] Gagal menyimpan: {str(e)}")

# --- 2. Simpan format HDF5 (.h5) ---
h5_path = os.path.join(save_dir, 'model_pneumonia.h5')
try:
    model.save(h5_path, save_format='h5')
    print(f"✅ Model disimpan dalam format .h5 di: {h5_path}")
except Exception as e:
    print(f"❌ Gagal menyimpan model dalam format H5: {e}")

# --- 3. Simpan format SavedModel (untuk TF Serving, TFLite, dsb) ---
savedmodel_path = os.path.join(save_dir, 'saved_model_pneumonia')
try:
    model.export(savedmodel_path)  # Keras 3 method
    print(f"✅ Model disimpan dalam format SavedModel di: {savedmodel_path}")
except Exception as e:
    print(f"❌ Gagal menyimpan dalam format SavedModel: {e}")

# --- 4. Simpan format TensorFlow.js (TFJS) ---
tfjs_path = os.path.join(save_dir, 'tfjs_model')
try:
    tfjs.converters.save_keras_model(model, tfjs_path)
    print(f"✅ Model disimpan dalam format TFJS di: {tfjs_path}")
except Exception as e:
    print(f"❌ Gagal menyimpan model dalam format TFJS: {e}")

# --- 5. Simpan format TFLite ---
tflite_path = os.path.join(save_dir, 'model_pneumonia.tflite')
try:
    converter = tf.lite.TFLiteConverter.from_keras_model(model)
    tflite_model = converter.convert()
    with open(tflite_path, 'wb') as f:
        f.write(tflite_model)
    print(f"✅ Model disimpan dalam format TFLite di: {tflite_path}")
except Exception as e:
    print(f"❌ Gagal menyimpan model dalam format TFLite: {e}")

# --- 6. Verifikasi (Opsional) ---
try:
    loaded_model = keras.models.load_model(keras_path)
    print(f"\n✅ Model .keras berhasil dimuat kembali dari: {keras_path}")
except Exception as e:
    print(f"\n❌ Gagal memuat kembali model .keras: {e}")